 # -*- coding: utf-8 -*-
"""HW4_309707001.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hJvOFuw0pguX8b_qyw11W7OjBzqi2REN
"""

from google.colab import drive
import os
drive.mount("/content/gdrive")
Path = os.getcwd() + '/gdrive/MyDrive/BigDataHW4'
!apt-get -y install openjdk-8-jre-headless
!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
from pyspark import SparkContext
import pandas as pd
import numpy as np
import gc
sc = SparkContext(appName="CORE")

file = ['2003.csv', '2004.csv', '2005.csv']
RDD1 = sc.textFile(Path + '/' + file[0])
RDD2 = sc.textFile(Path + '/' + file[1])
RDD3 = sc.textFile(Path + '/' + file[2])

COLS = RDD1.take(1)

rdd1 = RDD1.map(lambda feat: feat.split(",")).filter(lambda alist: alist[0] != 'Year')
rdd2 = RDD2.map(lambda feat: feat.split(",")).filter(lambda alist: alist[0] != 'Year')
rdd3 = RDD3.map(lambda feat: feat.split(",")).filter(lambda alist: alist[0] != 'Year')

for i,j in enumerate(COLS[0].split(',')):
  print(i, j, end = '-----')

rdd1 = rdd1.filter(lambda alist: 'NA' not in alist)
rdd2 = rdd2.filter(lambda alist: 'NA' not in alist)
rdd3 = rdd3.filter(lambda alist: 'NA' not in alist)

target1 = rdd1.map(lambda feat: [feat[1], feat[2], feat[3], feat[5], feat[7], feat[9], feat[18], int((int(feat[11]) - int(feat[12])) > 0)])
target2 = rdd2.map(lambda feat: [feat[1], feat[2], feat[3], feat[5], feat[7], feat[9], feat[18], int((int(feat[11]) - int(feat[12])) > 0)])
target3 = rdd3.map(lambda feat: [feat[1], feat[2], feat[3], feat[5], feat[7], feat[9], feat[18], int((int(feat[11]) - int(feat[12])) > 0)])

"""以下文字為資料內的所有特徵，以作為上面target的特徵參考。

0 Year
1 Month
2 DayofMonth
3 DayOfWeek
4 DepTime
5 CRSDepTime
6 ArrTime
7 CRSArrTime
8 UniqueCarrier
9 FlightNum
10 TailNum
11 ActualElapsedTime
12 CRSElapsedTime
13 AirTime
14 ArrDelay
15 DepDelay
16 Origin
17 Dest
18 Distance
19 TaxiIn
20 TaxiOut
21 Cancelled
22 CancellationCode
23 Diverted
24 CarrierDelay
25 WeatherDelay
26 NASDelay
27 SecurityDelay
28 LateAircraftDelay
29 Delay
"""

from pyspark.mllib.regression import LabeledPoint
f1 = target1.map(lambda features: LabeledPoint(features[-1], features[:-1]))
f2 = target2.map(lambda features: LabeledPoint(features[-1], features[:-1]))
f3 = target3.map(lambda features: LabeledPoint(features[-1], features[:-1]))

"""Validation Method（依據時間序列做類似K-Fold的validation）"""

f1.cache()
validate_clf = LogisticRegressionWithSGD.train(f1, iterations=100)
f2.cache()
validation_predict = f2.map(lambda rows: [float(validate_clf.predict(rows.features)), rows.label])
TP = validation_predict.filter(lambda sample:(sample[0]==1) and (sample[1]==1)).count()
TN = validation_predict.filter(lambda sample:(sample[0]==0) and (sample[1]==0)).count()
FP = validation_predict.filter(lambda sample:(sample[0]==1) and (sample[1]==0)).count()
FN = validation_predict.filter(lambda sample:(sample[0]==0) and (sample[1]==1)).count()
val_Acc = (TP + TN)/(TP + TN + FP + FN)
val_Pre = TP/(TP + FP + 0.001)
val_Rec = TP/(TP + FN + 0.001)
val_F1 = 2*val_Pre*val_Rec/(val_Pre + val_Rec + 0.001)

print("Accuracy:{0}, Precision:{1}, Recall:{2}, F1-Score{3}".format(val_Acc, val_Pre, val_Rec, val_F1))

f2.cache()
validate_clf = LogisticRegressionWithSGD.train(f2, iterations=100)
f3.cache()
validation_predict = f3.map(lambda rows: [float(validate_clf.predict(rows.features)), rows.label])
TP = validation_predict.filter(lambda sample:(sample[0]==1) and (sample[1]==1)).count()
TN = validation_predict.filter(lambda sample:(sample[0]==0) and (sample[1]==0)).count()
FP = validation_predict.filter(lambda sample:(sample[0]==1) and (sample[1]==0)).count()
FN = validation_predict.filter(lambda sample:(sample[0]==0) and (sample[1]==1)).count()
val_Acc = (TP + TN)/(TP + TN + FP + FN)
val_Pre = TP/(TP + FP + 0.001)
val_Rec = TP/(TP + FN + 0.001)
val_F1 = 2*val_Pre*val_Rec/(val_Pre + val_Rec + 0.001)
print("Accuracy:{0}, Precision:{1}, Recall:{2}, F1-Score{3}".format(val_Acc, val_Pre, val_Rec, val_F1))

print("Accuracy:{0}, Precision:{1}, Recall:{2}, F1-Score{3}".format(val_Acc, val_Pre, val_Rec, val_F1))

f1.cache()
validate_clf = LogisticRegressionWithSGD.train(f1, iterations=100)
f3.cache()
validation_predict = f3.map(lambda rows: [float(validate_clf.predict(rows.features)), rows.label])
TP = validation_predict.filter(lambda sample:(sample[0]==1) and (sample[1]==1)).count()
TN = validation_predict.filter(lambda sample:(sample[0]==0) and (sample[1]==0)).count()
FP = validation_predict.filter(lambda sample:(sample[0]==1) and (sample[1]==0)).count()
FN = validation_predict.filter(lambda sample:(sample[0]==0) and (sample[1]==1)).count()
val_Acc = (TP + TN)/(TP + TN + FP + FN)
val_Pre = TP/(TP + FP + 0.001)
val_Rec = TP/(TP + FN + 0.001)
val_F1 = 2*val_Pre*val_Rec/(val_Pre + val_Rec + 0.001)
print("Accuracy:{0}, Precision:{1}, Recall:{2}, F1-Score{3}".format(val_Acc, val_Pre, val_Rec, val_F1))

"""最終預測結果"""

from pyspark.mllib.classification import LogisticRegressionWithSGD
X_train = f1.union(f2)
X_train.cache()
clf = LogisticRegressionWithSGD.train(X_train, iterations=100)

f3.cache()
predictionAndLabels = f3.map(lambda rows: [float(clf.predict(rows.features)), rows.label])

TP = predictionAndLabels.filter(lambda sample:(sample[0]==1) and (sample[1]==1)).count()
TN = predictionAndLabels.filter(lambda sample:(sample[0]==0) and (sample[1]==0)).count()
FP = predictionAndLabels.filter(lambda sample:(sample[0]==1) and (sample[1]==0)).count()
FN = predictionAndLabels.filter(lambda sample:(sample[0]==0) and (sample[1]==1)).count()

Acc = (TP + TN)/(TP + TN + FP + FN)
Pre = TP/(TP + FP + 0.001)
Rec = TP/(TP + FN + 0.001)
F1 = 2*Pre*Rec/(Pre + Rec + 0.001)
print("Accuracy:{0}, Precision:{1}, Recall:{2}, F1-Score{3}".format(Acc, Pre, Rec, F1))

predictionAndLabels.take(10)